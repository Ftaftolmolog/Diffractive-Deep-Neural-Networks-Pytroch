{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.layers import Layer,Lambda,InputLayer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import cvnn.layers as complex_layers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall tensorflow\n",
    "# pip install tensorflow==2.9.0 cvnn tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset fashion_mnist/3.0.1 (download: Unknown size, generated: Unknown size, total: Unknown size) to ./data\\fashion_mnist\\3.0.1...\n",
      "Shuffling and writing examples to ./data\\fashion_mnist\\3.0.1.incompleteK9S8LJ\\fashion_mnist-train.tfrecord\n",
      "Shuffling and writing examples to ./data\\fashion_mnist\\3.0.1.incompleteK9S8LJ\\fashion_mnist-test.tfrecord\n",
      "Dataset fashion_mnist downloaded and prepared to ./data\\fashion_mnist\\3.0.1. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "size = 200\n",
    "BUFFER_SIZE = 5000\n",
    "BATCH_SIZE_PER_REPLICA = 8\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * 1\n",
    "\n",
    "# Define the traning parameters\n",
    "keep_training = False\n",
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "\n",
    "# load the dataset\n",
    "datasets, info = tfds.load(name='fashion_mnist', with_info=True, as_supervised=True, data_dir='./data')\n",
    "\n",
    "fashion_mnist_train, fashion_mnist_test = datasets['train'], datasets['test']\n",
    "\n",
    "num_train = info.splits['train'].num_examples\n",
    "num_test = info.splits['test'].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Define phase object ( Preprocess the images with up-sampling )\n",
    "1. Load image that serves as phase object  \n",
    "\n",
    "2. Image is 28x28 pixels, and is padded to 200x200 pixels  with 0's  \n",
    "\n",
    "3. Phase Image = exp(2$\\pi$ i * Padded Image)\n",
    "\n",
    "The digital image is encoded in phasor form, with an uniform amplitude and different phase angle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function preprocess at 0x000002138ADF2678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function preprocess at 0x000002138ADF2678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function preprocess at 0x000002138ADF2678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "def preprocess(image, label):\n",
    "    label = tf.one_hot(tf.cast(label, tf.int32), 10)   # convert the label to categorial, or one-hot coded\n",
    "    \n",
    "    up_sampling_size = int(1*size)\n",
    "    padding_size = (size - up_sampling_size)//2\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Step1: upsample the image to 120x120\n",
    "    up_sampling_image = tf.image.resize(image,\n",
    "                                        size=[up_sampling_size,up_sampling_size],\n",
    "                                        method='nearest')\n",
    "    up_sampling_image = up_sampling_image / 255.0\n",
    "    # Step2: get the phase object\n",
    "    phase_image = tf.math.exp(2*np.pi*1j*tf.cast(up_sampling_image,dtype=tf.complex64))\n",
    "    # Step3: pad the phase object to 200x200 with 0s\n",
    "    zero_padded_image = tf.pad(phase_image,\n",
    "                                paddings=[[padding_size,padding_size],[padding_size,padding_size],[0,0]],\n",
    "                                mode=\"CONSTANT\",constant_values=0)\n",
    "        \n",
    "    return tf.cast(zero_padded_image, dtype=tf.complex64), label\n",
    "    \n",
    "train_dataset = fashion_mnist_train.map(preprocess).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = fashion_mnist_test.map(preprocess).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Build the Diffraction layer using Angular Spectrum method\n",
    "\n",
    "### Angular Spectrum Propagation\n",
    "$$U_1(x,y) =\\mathcal{F}^{-1}[\\mathcal{F} U_0(x,y)\\mathcal{F}h(x,y)]$$\n",
    "\n",
    "$$U_1(x,y) =\\mathcal{F}^{-1}[\\mathcal{F} U_0(x,y) H(f_x,f_y)]$$\n",
    "\n",
    "\n",
    "This can be described using Fourier transforms.The first Fourier transform decomposes the initial field into plane waves. To propagate the plane waves, we multiply each wave by a complex phase factor, and then we take the inverse Fourier transform to add all the propagated plane waves back together.\n",
    "\n",
    "To implement Angular spectrum propagation, the Fouier transform of the initial field is first multiplied with the phase factor $$H=e^{ik_zz}$$, where $k_z$is a function of the spatial frequencies $$k_z=\\sqrt{k^2-k_x^2-k_y}$$where $$  k = \\frac{2\\pi}{\\lambda}$$ and $k_x$ and $k_y$ are related to the spatial frequencies $f_x$ and $f_y$ by a factor of $2\\pi$ $$k_{x,y} = 2\\pi f_{x,y}$$\n",
    "\n",
    "Hence the complex exponential can be written in terms of the Fourier coordinates $f_x$ and $f_y$, that is \n",
    "\n",
    "$$H=e^{ik_zz},k_z = 2\\pi \\sqrt{\\frac{1}{\\lambda}-f_x-f_y}$$\n",
    "\n",
    "Descretized spatial freqnency $f_x = k*\\Delta f = \\frac{k}{N \\Delta x}$  \n",
    "Interval between the spatial frequencies $\\Delta f = \\frac{1}{N \\Delta x}=\\frac{1}{L}$,where $L$ denotes the field of view in object space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffraction_Layer(Layer):\n",
    "    def __init__(self, units =200):\n",
    "        '''Initialize the diffraction layer attributes'''\n",
    "        super(Diffraction_Layer, self).__init__()\n",
    "        self.units = units\n",
    "        self.Nx = units              # Nx is the dimension of the grid\n",
    "        # self.L = 0.08              # source and observation plane side length, field of view\n",
    "        self.dx = 2e-6\n",
    "        self.lam = 7.5e-6          # wavelength of the optical wave\n",
    "        self.z = 3e-2                # distance of propagation(the distance bewteen two layers)\n",
    "   \n",
    "    def build(self, input_shape):\n",
    "        '''Create the state of the layer (weights)'''\n",
    "        phase_init = tf.random_normal_initializer()\n",
    "        self.phase = tf.Variable(name= \"phase\",\n",
    "                                initial_value=phase_init(shape=(self.units,self.units), dtype='float32'),\n",
    "                                trainable=True)\n",
    "                                # constraint=lambda t: 2*np.pi*tf.math.sigmoid(t))\n",
    "        # To help with the 3D-printing and fabrication of the D2NN design, \n",
    "        # a sigmoid function was used to constrain the phase value of each neuron\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        '''Define the computation'''\n",
    "        def angular_spectrum_propagator(E, z = self.z, lam = self.lam):\n",
    "            # compute angular spectrum\n",
    "            fft_c = tf.signal.fft2d(E)\n",
    "            c = tf.signal.fftshift(fft_c)\n",
    "\n",
    "            fx = np.fft.fftshift(np.fft.fftfreq(self.Nx, d = self.dx))\n",
    "            fxx, fyy = np.meshgrid(fx, fx)\n",
    "            argument = (2 * np.pi)**2 * ((1. / lam) ** 2 - fxx ** 2 - fyy ** 2)\n",
    "\n",
    "           #Calculate the propagating and the evanescent (complex) modes\n",
    "            tmp = np.sqrt(np.abs(argument))\n",
    "            kz = np.where(argument >= 0, tmp, 1j*tmp)\n",
    "\n",
    "            # propagate the angular spectrum a distance z\n",
    "            E = tf.signal.ifft2d(tf.signal.ifftshift(c * np.exp(1j * kz * z)))\n",
    "            return E\n",
    "        return tf.multiply(angular_spectrum_propagator(inputs),tf.math.exp(1j*tf.cast(self.phase,dtype=tf.complex64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notice\n",
    "For ___tf.cast()___: In case of casting from real types to complex types(complex64), the imaginary part of the returned value is set to 0.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Propogation(Layer):\n",
    "    def __init__(self, units =200):\n",
    "        '''Initialize the diffraction layer attributes'''\n",
    "        super(Propogation, self).__init__()\n",
    "        self.units = units\n",
    "        self.Nx = units              # Nx is the dimension of the grid\n",
    "        self.dx = 2e-6\n",
    "        self.lam = 7.5e-6            # wavelength of the optical wave\n",
    "        self.z = 1e-2                # distance of propagation(the distance bewteen last layer and the detector)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        '''Define the computation'''\n",
    "        def angular_spectrum_propagator(E, z = self.z, lam = self.lam):\n",
    "            # compute angular spectrum\n",
    "            fft_c = tf.signal.fft2d(E)\n",
    "            c = tf.signal.fftshift(fft_c)\n",
    "\n",
    "            fx = np.fft.fftshift(np.fft.fftfreq(self.Nx, d = self.dx))\n",
    "            fxx, fyy = np.meshgrid(fx, fx)\n",
    "            argument = (2 * np.pi)**2 * ((1. / lam) ** 2 - fxx ** 2 - fyy ** 2)\n",
    "\n",
    "           #Calculate the propagating and the evanescent (complex) modes\n",
    "            tmp = np.sqrt(np.abs(argument))\n",
    "            kz = np.where(argument >= 0, tmp, 1j*tmp)\n",
    "\n",
    "            # propagate the angular spectrum a distance z\n",
    "            E = tf.signal.ifft2d(tf.signal.ifftshift(c * np.exp(1j * kz * z))) # phase是加还是减\n",
    "            return E\n",
    "        return angular_spectrum_propagator(inputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(Layer):\n",
    "    def __init__(self, units=200):\n",
    "        '''Initialize the instance attributes'''\n",
    "        super(Detector, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        ''' Converts output to one hot form\n",
    "        Applies softmax'''\n",
    "        \n",
    "        def rang(arr,shape,size=size,base = 512):\n",
    "            x0 = shape[0] * size // base\n",
    "            y0 = shape[2] * size // base\n",
    "            delta = (shape[1]-shape[0])* size // base\n",
    "            return arr[x0:x0+delta,y0:y0+delta]\n",
    "        \n",
    "        def reduce_mean(tf_):\n",
    "            return tf.reduce_mean(tf_)\n",
    "        \n",
    "        def _ten_regions(a):\n",
    "            return tf.map_fn(reduce_mean,tf.convert_to_tensor([\n",
    "                rang(a,(120,170,120,170)),\n",
    "                rang(a,(120,170,240,290)),\n",
    "                rang(a,(120,170,360,410)),\n",
    "                rang(a,(220,270,120,170)),\n",
    "                rang(a,(220,270,200,250)),\n",
    "                rang(a,(220,270,280,330)),\n",
    "                rang(a,(220,270,360,410)),\n",
    "                rang(a,(320,370,120,170)),\n",
    "                rang(a,(320,370,240,290)),\n",
    "                rang(a,(320,370,360,410))\n",
    "            ]))\n",
    "        \n",
    "        def ten_regions(logits):\n",
    "            return tf.map_fn(_ten_regions,tf.abs(logits),dtype=tf.float32)\n",
    "\n",
    "        return tf.square(ten_regions(tf.abs(inputs))) # logits_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_label,logits_abs):\n",
    "    return tf.reduce_mean(tf.square(logits_abs-y_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Diffraction_Layer.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000002138B555CC8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Diffraction_Layer.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000002138B555CC8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Diffraction_Layer.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000002138B555CC8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Propogation.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000002138B63EF48>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Propogation.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000002138B63EF48>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Propogation.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x000002138B63EF48>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Detector.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000021388E56848>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Detector.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000021388E56848>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Detector.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x0000021388E56848>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From c:\\users\\cheng\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\cheng\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Detector.call.<locals>._ten_regions at 0x000002138B60C828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Detector.call.<locals>._ten_regions at 0x000002138B60C828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Detector.call.<locals>._ten_regions at 0x000002138B60C828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Detector.call.<locals>.reduce_mean at 0x000002138B60C678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Detector.call.<locals>.reduce_mean at 0x000002138B60C678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Detector.call.<locals>.reduce_mean at 0x000002138B60C678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "def get_D2NN_model():\n",
    "    inputs = complex_layers.complex_input(shape=(size,size))\n",
    "    h1 = Diffraction_Layer(size)(inputs)\n",
    "    h2 = Diffraction_Layer(size)(h1)\n",
    "    h3 = Diffraction_Layer(size)(h2)\n",
    "    h4 = Diffraction_Layer(size)(h3)\n",
    "    h5 = Diffraction_Layer(size)(h4)\n",
    "    propogation = Propogation(size)(h5)\n",
    "    out = Detector()(propogation)\n",
    "    return tf.keras.Model(inputs, out)\n",
    "\n",
    "\n",
    "D2NN = get_D2NN_model()\n",
    "\n",
    "# D2NN.summary()\n",
    "# plot_model(D2NN, show_shapes=True, show_layer_names=True, to_file='D2NN-model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "7500/7500 [==============================] - 500s 66ms/step - loss: 0.0493 - accuracy: 0.7656 - val_loss: 0.0384 - val_accuracy: 0.8080\n",
      "Epoch 2/15\n",
      "7500/7500 [==============================] - 452s 60ms/step - loss: 0.0370 - accuracy: 0.8179 - val_loss: 0.0371 - val_accuracy: 0.8087\n",
      "Epoch 3/15\n",
      "7500/7500 [==============================] - 460s 61ms/step - loss: 0.0360 - accuracy: 0.8252 - val_loss: 0.0359 - val_accuracy: 0.8212\n",
      "Epoch 4/15\n",
      "7500/7500 [==============================] - 461s 61ms/step - loss: 0.0355 - accuracy: 0.8284 - val_loss: 0.0361 - val_accuracy: 0.8187\n",
      "Epoch 5/15\n",
      "7500/7500 [==============================] - 464s 62ms/step - loss: 0.0352 - accuracy: 0.8298 - val_loss: 0.0362 - val_accuracy: 0.8221\n",
      "Epoch 6/15\n",
      "7500/7500 [==============================] - 461s 61ms/step - loss: 0.0351 - accuracy: 0.8313 - val_loss: 0.0357 - val_accuracy: 0.8219\n",
      "Epoch 7/15\n",
      "7500/7500 [==============================] - 469s 62ms/step - loss: 0.0349 - accuracy: 0.8343 - val_loss: 0.0355 - val_accuracy: 0.8213\n",
      "Epoch 8/15\n",
      "7500/7500 [==============================] - 477s 64ms/step - loss: 0.0348 - accuracy: 0.8328 - val_loss: 0.0355 - val_accuracy: 0.8226\n",
      "Epoch 9/15\n",
      "7500/7500 [==============================] - 461s 62ms/step - loss: 0.0347 - accuracy: 0.8342 - val_loss: 0.0358 - val_accuracy: 0.8161\n",
      "Epoch 10/15\n",
      "7500/7500 [==============================] - 461s 61ms/step - loss: 0.0346 - accuracy: 0.8356 - val_loss: 0.0353 - val_accuracy: 0.8177\n",
      "Epoch 11/15\n",
      "7500/7500 [==============================] - 452s 60ms/step - loss: 0.0346 - accuracy: 0.8344 - val_loss: 0.0353 - val_accuracy: 0.8261\n",
      "Epoch 12/15\n",
      "7500/7500 [==============================] - 469s 62ms/step - loss: 0.0345 - accuracy: 0.8357 - val_loss: 0.0361 - val_accuracy: 0.8181\n",
      "Epoch 13/15\n",
      "7500/7500 [==============================] - 469s 62ms/step - loss: 0.0344 - accuracy: 0.8355 - val_loss: 0.0355 - val_accuracy: 0.8178\n",
      "Epoch 14/15\n",
      "7500/7500 [==============================] - 466s 62ms/step - loss: 0.0344 - accuracy: 0.8358 - val_loss: 0.0355 - val_accuracy: 0.8219\n",
      "Epoch 15/15\n",
      "7500/7500 [==============================] - 466s 62ms/step - loss: 0.0344 - accuracy: 0.8368 - val_loss: 0.0351 - val_accuracy: 0.8243\n"
     ]
    }
   ],
   "source": [
    "if keep_training:\n",
    "    D2NN.load_weights('./training_results/D2NN_phase_only')\n",
    "\n",
    "D2NN.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "             loss=loss_function,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "checkpoint_path = './training_results/D2NN_phase_only'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                             save_weights_only=True,\n",
    "                             sace_freq='epoch')\n",
    "\n",
    "history = D2NN.fit(train_dataset,\n",
    "                   epochs=epochs,\n",
    "                   validation_data=test_dataset,\n",
    "                   callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extracting weights from model\n",
    "- Neurons’ phase values were converted into a relative height map (Δ𝑧=𝜆$\\phi$/2𝜋Δ𝑛)    \n",
    "  ,where Δ𝑛 is the refractive index difference between the 3D printing material (VeroBlackPlus RGD875) and air\n",
    "- save the height map to a numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max value in wights is: 6.2831736\n",
      "The mim value in wights is: 3.5052653e-05\n"
     ]
    }
   ],
   "source": [
    "D2NN.load_weights('./training_results/D2NN_phase_only')\n",
    "# print(D2NN.layers)\n",
    "# print(D2NN.layers[1].weights)\n",
    "# print(D2NN.layers[5].get_weights()) # get the numpy arrays for the parameters of the layer\n",
    "# print(D2NN.get_layer('diffraction__layer_4').phase)\n",
    "\n",
    "# Extract all the weights from the model\n",
    "weights = []\n",
    "for l in range(1,5+1):\n",
    "    weights.append(np.squeeze(D2NN.layers[l].get_weights()))\n",
    "    \n",
    "# Map the weights into range [0, 2*pi]\n",
    "for l in range(0,5):\n",
    "    for i in range(0,200):\n",
    "        for j in range(0,200):\n",
    "            while (weights[l][i][j] < 0):\n",
    "                    weights[l][i][j] += 2*np.pi\n",
    "            while(weights[l][i][j] > 2*np.pi):\n",
    "                weights[l][i][j] -= 2*np.pi\n",
    "print(\"The max value in wights is: \" + str(np.max(weights)))\n",
    "print(\"The mim value in wights is: \" + str(np.min(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the height map for all layers: (5, 200, 200)\n",
      "Max and min value in height are: 1.0382041e-05,5.791947e-11\n",
      "\n",
      "height_map.npy has saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Convert the weights to height map\n",
    "lam = 7.5e-6\n",
    "\n",
    "material_refractive_index = 1.7227 # VeroBlackPlus RGD875\n",
    "air_refractive_index = 1.0003\n",
    "delta_n = material_refractive_index - air_refractive_index\n",
    "\n",
    "height_map = (lam*np.array(weights)) / (2*np.pi*delta_n)         \n",
    "\n",
    "# Check the shape and save it to np file\n",
    "print(\"The shape of the height map for all layers:\",end=' ')\n",
    "print(np.shape(np.array(height_map)))\n",
    "print(\"Max and min value in height are: \" + str(np.max(height_map)) + \",\"+ str(np.min(height_map))+\"\\n\")\n",
    "\n",
    "np.save('height_map.npy',np.array(height_map))\n",
    "print(\"height_map.npy has saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
